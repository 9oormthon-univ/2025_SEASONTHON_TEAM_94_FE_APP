package com.example.stopusing_app.ai

import android.content.Context
import android.content.res.AssetManager
import android.util.Log
import com.example.stopusing_app.config.KoreanFinancialVocabulary
import org.tensorflow.lite.Interpreter
import java.io.FileInputStream
import java.nio.ByteBuffer
import java.nio.channels.FileChannel

/**
 * TensorFlow Lite ëª¨ë¸ ê´€ë¦¬ í´ë˜ìŠ¤
 * ëª¨ë¸ ë¡œë”©, ì´ˆê¸°í™”, ì¶”ë¡  ì‹¤í–‰ì„ ë‹´ë‹¹
 */
class TensorFlowLiteManager(private val context: Context) {
    
    companion object {
        private const val TAG = "TensorFlowLiteManager"
    }
    
    private var tfliteInterpreter: Interpreter? = null
    
    /**
     * TensorFlow Lite ëª¨ë¸ ì´ˆê¸°í™”
     * 
     * @return ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
     */
    fun initializeModel(): Boolean {
        return try {
            Log.d(TAG, "ğŸ¤– Initializing TensorFlow Lite model...")
            
            if (!isModelAvailable()) {
                Log.w(TAG, "âš ï¸ Model file not found: ${KoreanFinancialVocabulary.MODEL_FILE}")
                return false
            }
            
            val modelBuffer = loadModelFile()
            val options = Interpreter.Options().apply {
                setNumThreads(2) // CPU ìµœì í™”
                setUseNNAPI(true) // Android Neural Networks API í™œìš©
            }
            
            tfliteInterpreter = Interpreter(modelBuffer, options)
            Log.d(TAG, "âœ… TensorFlow Lite model initialized successfully")
            true
            
        } catch (e: Exception) {
            Log.e(TAG, "ğŸ’¥ Failed to initialize TensorFlow Lite model", e)
            tfliteInterpreter = null
            false
        }
    }
    
    /**
     * ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
     * 
     * @param inputBuffer ì…ë ¥ ë°ì´í„° ë²„í¼ (character-level tokenized input)
     * @return ì¶”ë¡  ê²°ê³¼ ë°°ì—´ [batch_size=1, sequence_length=200, num_classes=7]
     */
    fun runInference(inputBuffer: ByteBuffer): Array<Array<FloatArray>>? {
        val interpreter = tfliteInterpreter ?: run {
            Log.e(TAG, "ğŸ’¥ TensorFlow Lite interpreter not initialized")
            return null
        }
        
        return try {
            // ë²”ìš© ëª¨ë¸ ì¶œë ¥ í˜•íƒœ: [1, 200, 7]
            val outputBuffer = Array(1) { 
                Array(KoreanFinancialVocabulary.MAX_SEQUENCE_LENGTH) { 
                    FloatArray(7) 
                } 
            }
            
            interpreter.run(inputBuffer, outputBuffer)
            Log.d(TAG, "ğŸ¤– Universal model inference completed successfully")
            outputBuffer
            
        } catch (e: Exception) {
            Log.e(TAG, "ğŸ’¥ Universal model inference failed", e)
            null
        }
    }
    
    /**
     * ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
     */
    fun isInitialized(): Boolean = tfliteInterpreter != null
    
    /**
     * ëª¨ë¸ íŒŒì¼ì´ assetsì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
     */
    private fun isModelAvailable(): Boolean {
        return try {
            context.assets.open(KoreanFinancialVocabulary.MODEL_FILE).close()
            true
        } catch (e: Exception) {
            false
        }
    }
    
    /**
     * assetsì—ì„œ ëª¨ë¸ íŒŒì¼ ë¡œë“œ
     */
    private fun loadModelFile(): ByteBuffer {
        val assetManager: AssetManager = context.assets
        val fileDescriptor = assetManager.openFd(KoreanFinancialVocabulary.MODEL_FILE)
        val inputStream = FileInputStream(fileDescriptor.fileDescriptor)
        val fileChannel = inputStream.channel
        val startOffset = fileDescriptor.startOffset
        val declaredLength = fileDescriptor.declaredLength
        
        Log.d(TAG, "ğŸ“ Loading model file: ${KoreanFinancialVocabulary.MODEL_FILE} (${declaredLength} bytes)")
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)
    }
    
    /**
     * ë¦¬ì†ŒìŠ¤ ì •ë¦¬
     */
    fun cleanup() {
        try {
            tfliteInterpreter?.close()
            tfliteInterpreter = null
            Log.d(TAG, "ğŸ§¹ TensorFlow Lite resources cleaned up")
        } catch (e: Exception) {
            Log.e(TAG, "ğŸ’¥ Error during cleanup", e)
        }
    }
    
    /**
     * ëª¨ë¸ ì •ë³´ ë¡œê¹…
     */
    fun logModelInfo() {
        val interpreter = tfliteInterpreter ?: return
        
        try {
            val inputTensor = interpreter.getInputTensor(0)
            val outputTensor = interpreter.getOutputTensor(0)
            
            Log.d(TAG, "ğŸ“Š Model Info:")
            Log.d(TAG, "  Input shape: ${inputTensor.shape().contentToString()}")
            Log.d(TAG, "  Output shape: ${outputTensor.shape().contentToString()}")
            Log.d(TAG, "  Input type: ${inputTensor.dataType()}")
            Log.d(TAG, "  Output type: ${outputTensor.dataType()}")
            
        } catch (e: Exception) {
            Log.e(TAG, "ğŸ’¥ Error getting model info", e)
        }
    }
}